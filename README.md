# Ollama-Local-LLM
Mini Assignment: Local LLM Deployment and Interaction


## Instructions

### MacOS

Download ollama dmg file from https://ollama.com/. Follow steps to install in the app.

### Linux

Install with one command

```sh
curl -fsSL https://ollama.com/install.sh | sh
```

## Run LLms Locally

### Llama3
1. Find the model parameter that you want to run from the Ollama website https://ollama.com/library/llama3
2. Run the model
   ```sh
   ollama run llama3    # this runs the 8b parameter model
   ```

### Mistral
1. Find the model parameter that you want to run from the Ollama website https://ollama.com/library/mistral
2. Run the model
   ```sh
   ollama run mistral    # this runs the 7b parameter model
   ```

## Benchmarks mac m2 vs nvidia rtx 2080 super


https://github.com/jainal09/Ollama-Local-LLM/assets/34179361/ec35956a-97ce-4a8c-aa1d-02c7f3c6530d

### Miscellaneous 

### Running Mistral 7b model to create a 3d ascii donut in python (Attempt [not successful]! ðŸ«¡)

https://github.com/jainal09/Ollama-Local-LLM/assets/34179361/40b36e86-9203-43ea-8360-48ae4fce8a15





